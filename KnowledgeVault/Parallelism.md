# Parallelism

In computer engineering, the term parallelism refers to any situation in which two or more distinct hardware components are operating simultaneously. In other words, parallel computer hardware can perform more than one task at a time. In contrast, serial computer hardware is capable of doing only one thing at a time.

Prior to 1989, consumer-grade computing devices were exclusively serial machines. Examples include the MOS Technology 6502 CPU, which was used in the Apple II and Commodore 64 personal computers, and the Intel 8086, 80286 and 80386 CPUs which were at the heart of early IBM PCs and clones.

Today, parallel computing hardware is ubiquitous. One obvious example of hardware parallelism is a multicore CPU, such as the Intel Core™ i7 or the AMD Ryzen™ 7. But parallelism can be employed at a wide range of granularities. For example, a single [[CPU]] might contain multiple [[ALU]]s and therefore be capable of performing multiple independent calculations in parallel. And at the other end of the spectrum, a cluster of computers working in tandem to solve a common problem is also an example of hardware parallelism.

[[Implicit versus Explicit Parallelism]]
[[Task versus Data Parallelism]]